#!/usr/bin/env python3
"""
æ‰«æå·¥ä½œåŒºä¸­çš„æ‰€æœ‰é£ä¹¦æ–‡æ¡£é“¾æ¥
æå–æ–‡æ¡£ token å’Œç›¸å…³ä¿¡æ¯
"""

import os
import re
from pathlib import Path
from typing import List, Dict

def scan_workspace_for_feishu_docs(workspace_path: str) -> List[Dict]:
    """
    æ‰«æå·¥ä½œåŒºï¼ŒæŸ¥æ‰¾æ‰€æœ‰é£ä¹¦æ–‡æ¡£é“¾æ¥
    """
    feishu_docs = []
    
    # é£ä¹¦æ–‡æ¡£é“¾æ¥çš„æ­£åˆ™è¡¨è¾¾å¼
    patterns = [
        r'https://[a-z0-9-]+\.feishu\.cn/docx/([a-zA-Z0-9]+)',
        r'https://[a-z0-9-]+\.feishu\.cn/wiki/([a-zA-Z0-9]+)',
        r'docx/([a-zA-Z0-9]+)',
        r'wiki/([a-zA-Z0-9]+)',
    ]
    
    combined_pattern = '|'.join(f'({p})' for p in patterns)
    
    # éå†å·¥ä½œåŒº
    for root, dirs, files in os.walk(workspace_path):
        # è·³è¿‡éšè—ç›®å½•å’Œç‰¹å®šç›®å½•
        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', '.git']]
        
        for file in files:
            if not file.endswith('.md'):
                continue
                
            file_path = os.path.join(root, file)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„é“¾æ¥
                matches = re.finditer(combined_pattern, content)
                
                for match in matches:
                    # æå– token
                    token = None
                    for group in match.groups():
                        if group and len(group) > 5 and re.match(r'^[a-zA-Z0-9]+$', group):
                            token = group
                            break
                    
                    if token:
                        feishu_docs.append({
                            'token': token,
                            'file': file_path.replace(workspace_path, ''),
                            'link': match.group(0)
                        })
                        
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
    
    return feishu_docs

def main():
    workspace = '/home/ubuntu/.openclaw/workspace'
    
    print("ğŸ” æ‰«æå·¥ä½œåŒºä¸­çš„é£ä¹¦æ–‡æ¡£...")
    docs = scan_workspace_for_feishu_docs(workspace)
    
    if not docs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•é£ä¹¦æ–‡æ¡£")
        return
    
    # å»é‡
    unique_docs = {}
    for doc in docs:
        token = doc['token']
        if token not in unique_docs:
            unique_docs[token] = doc
        else:
            # è®°å½•é‡å¤å¼•ç”¨
            unique_docs[token]['file'] += f", {doc['file']}"
    
    print(f"\nâœ… æ‰¾åˆ° {len(unique_docs)} ä¸ªå”¯ä¸€çš„é£ä¹¦æ–‡æ¡£:\n")
    
    for i, (token, doc) in enumerate(unique_docs.items(), 1):
        print(f"{i}. Token: {token}")
        print(f"   å¼•ç”¨ä½ç½®: {doc['file']}")
        print(f"   é“¾æ¥: {doc['link'][:80]}..." if len(doc['link']) > 80 else f"   é“¾æ¥: {doc['link']}")
        print()
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = '/home/ubuntu/.openclaw/workspace/feishu_docs_inventory.json'
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_docs, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‹ æ¸…å•å·²ä¿å­˜åˆ°: {output_file}")

if __name__ == '__main__':
    main()
